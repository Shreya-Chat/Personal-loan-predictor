# -*- coding: utf-8 -*-
"""RandomForest_Loan_prediction_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y3a6OaJ7IYJvRxLRUsmper4W8SYZeXwg
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as sts
import warnings
warnings.filterwarnings("ignore")

df=pd.read_csv("bankloan.csv")
df=df.drop(["ID"],axis=1)
df.shape

df.head()

df.size

df.isnull().sum()

df.duplicated().sum()

df.describe()

df=df[df["Experience"]>=0]

cor=df.corr()
plt.figure(figsize=(14,7))
sns.heatmap(cor,annot=True)

df__0=df[df["Personal.Loan"]==0]
df_1=df[df["Personal.Loan"]==1]
print(f'The number of people got the loan are {len(df_1)}')
print(f'The number of people not got the loan are {len(df__0)}')
df_0=df__0.sample(n=500,random_state=42)
df_final=pd.concat([df_0,df_1],axis=0)
print()
print("After balancing the data")
print(f'The number of people got the loan are {len(df_1)}')
print(f'The number of people not got the loan are {len(df_0)}')

"""**Data was imbalances so we have balanced the data by randomly select the instances for people who didn't get the loan.**"""

corr=df_final.corr()
plt.figure(figsize=(14,7))
sns.heatmap(corr,annot=True)

"""1. **We have dropped "Experience","Zip code","Online Banking" and "Credit card"
features due to very low correlation with target variable.**
"""

df=df_final.drop(["Experience","ZIP.Code","Online","CreditCard"],axis=1)
df=df.reset_index(drop=True)
df.head()

cor=df.corr()
plt.figure(figsize=(14,7))
sns.heatmap(cor,annot=True)

cor=df.corr()
df_cor=cor.copy()
for i in cor.columns:
  df_cor[i]=np.where((cor[i]>=.50) | (cor[i]<=-0.50),cor[i],np.nan)


plt.figure(figsize=(14,7))
sns.heatmap(df_cor,annot=True)

"""1. **Personal loan is highly correlated with income and CC Avg,**
2. **CCavg and Income has high multicolinearity.**
"""

df.columns

skewness={}
for i in df.columns[0:2].append(df.columns[3:4].append(df.columns[5:6])):
  val=df[i].skew()
  skewness[i]=val

pd.DataFrame([skewness])

"""**"Age" and "Income" features are Normally distributed while, "CCavg" and "Mortage" are Right skewed**"""

for i in df.columns[0:2].append(df.columns[3:4].append(df.columns[5:6])):
  plt.figure(figsize=(27,5))
  plt.subplot(131)
  sns.distplot(df[i])
  plt.title(f"Distplot of {i}")

  plt.subplot(132)
  sts.probplot(df[i],plot=plt)
  plt.title(f"Q-Q plot of {i}")
  plt.subplot(133)
  sns.boxplot(df[i])
  plt.title(f"Box Plot of {i}")

  plt.show()

"""**"CCAvg" and "Mortage" features consists outliers**"""

plt.figure(figsize=(27,5))
plt.subplot(141)
sns.distplot(df[df["Personal.Loan"]==0]["Family"],label="Not approved",hist=False)
sns.distplot(df[df["Personal.Loan"]==1]["Family"],label="Approved",hist=False)
plt.legend()
plt.title("distplot of Family")

plt.subplot(142)
sns.distplot(df[df["Personal.Loan"]==0]["Education"],label="Not approved",hist=False)
sns.distplot(df[df["Personal.Loan"]==1]["Education"],label="Approved",hist=False)
plt.legend()
plt.title("distplot of Education")

plt.subplot(143)
sns.distplot(df[df["Personal.Loan"]==0]["CD.Account"],label="Not approved",hist=False)
sns.distplot(df[df["Personal.Loan"]==1]["CD.Account"],label="Approved",hist=False)
plt.legend()
plt.title("distplot of CD.Account")

plt.subplot(144)
sns.distplot(df[df["Personal.Loan"]==0]["Securities.Account"],label="Not approved",hist=False)
sns.distplot(df[df["Personal.Loan"]==1]["Securities.Account"],label="Approved",hist=False)
plt.legend()
_=plt.title("distplot of Securities.Account")

"""1. **Person with family members 1 is most likely to get disapproval for loan.**
2. **Highly educated persons are most likely to get loan.**
3.  **Person with CD account are most likely to get loan.**
4. **Person without Security account are likely to get disapproval for loan.**
"""

cat_count_Family=df["Family"].value_counts()
cat_count_Education=df["Education"].value_counts()
cat_count_CDAcc=df["CD.Account"].value_counts()
cat_count_SecurityACC=df["Securities.Account"].value_counts()


cat_list=[cat_count_Family,cat_count_Education,cat_count_CDAcc,cat_count_SecurityACC]
plt.figure(figsize=(30,8))
plt.subplot(141)
_=plt.pie(cat_list[0],labels=cat_list[0].index,autopct='%1.1f%%', startangle=90,colors=sns.color_palette("Set1", len(cat_list[0])))
plt.title(f"Pie Chart of Family")

plt.subplot(142)
_=plt.pie(cat_list[1],labels=cat_list[1].index,autopct='%1.1f%%', startangle=90,colors=sns.color_palette("Set1", len(cat_list[0])))
plt.title(f"Pie Chart of Education")

plt.subplot(143)
_=plt.pie(cat_list[2],labels=cat_list[2].index,autopct='%1.1f%%', startangle=90,colors=sns.color_palette("Set1", len(cat_list[0])))
plt.title(f"Pie Chart of CD.Account")

plt.subplot(144)
_=plt.pie(cat_list[3],labels=cat_list[3].index,autopct='%1.1f%%', startangle=90,colors=sns.color_palette("Set1", len(cat_list[0])))
_=plt.title(f"Pie Chart of Securities.Account")

"""**Here, we can see the distribution of Family, Education,CD Account,Security Accounts**
1. There are most of the people without CD account and Security Accounts.
"""

sns.pairplot(df,hue="Personal.Loan",palette="Set1")

"""**People with high income, CC Avg, Mortage and Eduction level are most likely to get a Personal loan**"""

# from ydata_profiling import ProfileReport
# prof=ProfileReport(df)
# prof.to_file(output_file="bankloan.html")

df_num_col=df.select_dtypes(include="number").columns
def outliers(df, df_num_col):
    lower_lim = []
    upper_lim = []
    outlier_num = []
    for col in df_num_col:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        Lower_lim = round((Q1 - 1.5 * IQR), 0)
        Upper_lim = round((Q3 + 1.5 * IQR), 0)
        lower_lim.append(Lower_lim)
        upper_lim.append(Upper_lim)
        outliers_in_column = df[(df[col] < Lower_lim) | (df[col] > Upper_lim)]
        outlier_num.append(len(outliers_in_column))
    outlier_df = pd.DataFrame({
        'Column': df_num_col,
        'Lower Limit': lower_lim,
        'Upper Limit': upper_lim,
        'No of Outliers': outlier_num
    })

    return outlier_df



outliers(df,df_num_col)

for i in df_num_col[3:6]:
  Q1=df[i].quantile(0.25)
  Q3=df[i].quantile(0.75)
  IQR=Q3-Q1
  lower_lim=Q1-1.5*IQR
  upper_lim=Q3+1.5*IQR
  filt_df=df[(df[i]>=lower_lim) & (df[i]<=upper_lim)]


outliers(filt_df,df_num_col)

"""**After applying IQR method for trimming outliers and get rid of outliers.**"""

filt_df.isnull().sum()

x=filt_df.drop(["Personal.Loan"],axis=1)
y=filt_df["Personal.Loan"]
from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(x,y,test_size=0.2,random_state=42)
print(f"The shape of X_train is {X_train.shape}\n The shape of X_test is {X_test.shape}\n The shape of Y_train is {Y_train.shape} \n The shape of Y_test is {Y_test.shape}")

from sklearn.preprocessing import FunctionTransformer
trf=FunctionTransformer(func=np.log1p)
X_train_trans=X_train.copy()
X_test_trans=X_test.copy()
X_train_trans["Mortgage"]=trf.fit_transform(X_train["Mortgage"])
X_test_trans["Mortgage"]=trf.transform(X_test["Mortgage"])
X_train_trans.head()
outliers(X_train_trans,df_num_col[0:6])

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
X_train_scaled=pd.DataFrame(scaler.fit_transform(X_train),columns=X_train.columns)
X_test_scaled=pd.DataFrame(scaler.transform(X_test),columns=X_train.columns)
X_train_scaled.head()

skewness={}
for i in df.columns[0:2].append(df.columns[3:4].append(df.columns[5:6])):
  val=X_train_trans[i].skew()
  skewness[i]=val

pd.DataFrame([skewness])

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
para={"n_estimators":[10,20,30,40,50],"max_depth":[10,20,30],"max_features":["auto","log2"]}
model_rf=RandomForestClassifier()
clf=GridSearchCV(model_rf,para,cv=5)
clf.fit(X_train_trans,Y_train)
best_para=clf.best_params_
best_score=round(clf.best_score_,3)
print(f'The Best paramter are {best_para}\n The best Score is {best_score*100}%')

trans_model=RandomForestClassifier(max_depth=30,max_features="log2",n_estimators=10,random_state=42)
trans_model.fit(X_train_trans,Y_train)
y_pred=trans_model.predict(X_test_trans)

score=round(trans_model.score(X_test_trans,Y_test),3)
print(f'Accuracy is {score*100}%')

from sklearn.metrics import confusion_matrix
con=confusion_matrix(Y_test,y_pred)
sns.heatmap(con,annot=True,fmt="d")
plt.xlabel("Predicted")
plt.title("Confusion Matrix")
_=plt.ylabel("Truth")

from sklearn.metrics import classification_report
print("Classification Report")
print(classification_report(Y_test,y_pred))